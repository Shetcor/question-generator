{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Question Generator (Streamlit).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33fyuQ9eTfHB"
      },
      "source": [
        "# **R181600B - SHEUNESU C. TAZVIVINGA**\n",
        "# **R181558W - TAFADZWA N. YEMEKE**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Od_tRfdCiY5b"
      },
      "source": [
        "**Question generator**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSH_nS48peNa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef36f387-783d-476e-93b3-8947543537d5"
      },
      "source": [
        "!pip install -q streamlit"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 7.8MB 7.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 174kB 43.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 81kB 9.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.2MB 41.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 112kB 54.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 9.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 122kB 54.8MB/s \n",
            "\u001b[?25h  Building wheel for blinker (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: ipykernel 6.0.1 has requirement ipython>=7.23.1, but you'll have ipython 5.5.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement ipykernel~=4.10, but you'll have ipykernel 6.0.1 which is incompatible.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tL1NL38rXNP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba09613e-0a56-48f4-920e-c6e6a2400c7b"
      },
      "source": [
        "!pip -q install pyngrok"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |▍                               | 10kB 17.3MB/s eta 0:00:01\r\u001b[K     |▉                               | 20kB 23.2MB/s eta 0:00:01\r\u001b[K     |█▎                              | 30kB 18.6MB/s eta 0:00:01\r\u001b[K     |█▊                              | 40kB 15.6MB/s eta 0:00:01\r\u001b[K     |██▏                             | 51kB 7.2MB/s eta 0:00:01\r\u001b[K     |██▋                             | 61kB 7.1MB/s eta 0:00:01\r\u001b[K     |███                             | 71kB 8.1MB/s eta 0:00:01\r\u001b[K     |███▌                            | 81kB 8.4MB/s eta 0:00:01\r\u001b[K     |████                            | 92kB 8.6MB/s eta 0:00:01\r\u001b[K     |████▍                           | 102kB 6.9MB/s eta 0:00:01\r\u001b[K     |████▉                           | 112kB 6.9MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 122kB 6.9MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 133kB 6.9MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 143kB 6.9MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 153kB 6.9MB/s eta 0:00:01\r\u001b[K     |███████                         | 163kB 6.9MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 174kB 6.9MB/s eta 0:00:01\r\u001b[K     |████████                        | 184kB 6.9MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 194kB 6.9MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 204kB 6.9MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 215kB 6.9MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 225kB 6.9MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 235kB 6.9MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 245kB 6.9MB/s eta 0:00:01\r\u001b[K     |███████████                     | 256kB 6.9MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 266kB 6.9MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 276kB 6.9MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 286kB 6.9MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 296kB 6.9MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 307kB 6.9MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 317kB 6.9MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 327kB 6.9MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 337kB 6.9MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 348kB 6.9MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 358kB 6.9MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 368kB 6.9MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 378kB 6.9MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 389kB 6.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 399kB 6.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 409kB 6.9MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 419kB 6.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 430kB 6.9MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 440kB 6.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 450kB 6.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 460kB 6.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 471kB 6.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 481kB 6.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 491kB 6.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 501kB 6.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 512kB 6.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 522kB 6.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 532kB 6.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 542kB 6.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 552kB 6.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 563kB 6.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 573kB 6.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 583kB 6.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 593kB 6.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 604kB 6.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 614kB 6.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 624kB 6.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 634kB 6.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 645kB 6.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 655kB 6.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 665kB 6.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 675kB 6.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 686kB 6.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 696kB 6.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 706kB 6.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 716kB 6.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 727kB 6.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 737kB 6.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 747kB 6.9MB/s \n",
            "\u001b[?25h  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVBr4ERZrfzI"
      },
      "source": [
        "from pyngrok import ngrok"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTwHVaAmrjVD",
        "outputId": "2dcdf9ec-03db-47d1-a6b7-38d8fa1df9df"
      },
      "source": [
        "!ngrok authtoken 1npJviy2rm2H1RzUbevBUv2GLGw_ep9y73PeZMfA5zeSBzzs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CI_thqItr2mt",
        "outputId": "e1c872f9-7f2b-4815-9ba4-f294e5254651"
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7u0dNAu8HG08"
      },
      "source": [
        "#!pip uninstall python-crfsuite"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jr3UrbVs0bYE",
        "outputId": "4a88e442-28ce-46e2-f871-e216f15343f5"
      },
      "source": [
        "!pip install python-crfsuite"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting python-crfsuite\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/47/58f16c46506139f17de4630dbcfb877ce41a6355a1bbf3c443edb9708429/python_crfsuite-0.9.7-cp37-cp37m-manylinux1_x86_64.whl (743kB)\n",
            "\u001b[K     |████████████████████████████████| 747kB 6.8MB/s \n",
            "\u001b[?25hInstalling collected packages: python-crfsuite\n",
            "Successfully installed python-crfsuite-0.9.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Iu-TTbR1OFs",
        "outputId": "5893460b-29db-421c-8cbd-876bc4876d9c"
      },
      "source": [
        "!pip install sklearn-pycrfsuite"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sklearn-pycrfsuite\n",
            "  Downloading https://files.pythonhosted.org/packages/17/1c/d80272229ab530d05a157113908c707e642cd1e710e1d7b2bd6fd1e708dc/sklearn-pycrfsuite-0.4.0.tar.gz\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.7/dist-packages (from sklearn-pycrfsuite) (4.41.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sklearn-pycrfsuite) (1.15.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from sklearn-pycrfsuite) (0.8.9)\n",
            "Collecting python-crfsuite-extension\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/93/a8/f58b1b803fe9f3a96b501aa5558320fa8c3a3f80b715278bc32a1831ae3a/python-crfsuite-extension-0.9.7.tar.gz (485kB)\n",
            "\u001b[K     |████████████████████████████████| 491kB 9.7MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: sklearn-pycrfsuite, python-crfsuite-extension\n",
            "  Building wheel for sklearn-pycrfsuite (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sklearn-pycrfsuite: filename=sklearn_pycrfsuite-0.4.0-py2.py3-none-any.whl size=11004 sha256=5f568a708cb7f141524a81f63cf0aec004b1eb525ad6ccc5f12abc9ec48cd78c\n",
            "  Stored in directory: /root/.cache/pip/wheels/af/78/bd/189a8a1d7a6031ce1e1f8e81a5a8bdb0d3fba7e88c53f96107\n",
            "  Building wheel for python-crfsuite-extension (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-crfsuite-extension: filename=python_crfsuite_extension-0.9.7-cp37-cp37m-linux_x86_64.whl size=776116 sha256=4e5187553d6466e42d700e29dd74ae0143800f56c245e0e83ef4b5d2650b2df0\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ee/fa/10e4ae5dd9e71190ce1318fa7e2b82314bc880da1ec34ee567\n",
            "Successfully built sklearn-pycrfsuite python-crfsuite-extension\n",
            "Installing collected packages: python-crfsuite-extension, sklearn-pycrfsuite\n",
            "Successfully installed python-crfsuite-extension-0.9.7 sklearn-pycrfsuite-0.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-uwxxiKr5u_",
        "outputId": "8f76db3e-6774-48e5-9dac-5f34475575fc"
      },
      "source": [
        "%%writefile question_gen.py\n",
        "import streamlit as st\n",
        "import string\n",
        "import pycrfsuite\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tag import CRFTagger\n",
        "from nltk import word_tokenize,sent_tokenize\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "import time\n",
        "\n",
        "\n",
        "#Import PyTorch Framework\n",
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "ct = CRFTagger()\n",
        "nlp = spacy.load('en')\n",
        "\n",
        "PAGE_CONFIG = {\"page_title\":\"StColab.io\", \"page_icon\":\":smiley:\",\"layout\":\"centered\"}\n",
        "st.set_page_config(\"PAGE_CONFIG\")\n",
        "\n",
        "st.sidebar.title(\"Dashboard\")\n",
        "st.title(\"Question Generator\")\n",
        "\n",
        "page = st.sidebar.selectbox(\"Choose action\", [\"Home\", \"Generate Questions\"])\n",
        "def main():\n",
        "  if page == \"Home\":\n",
        "    st.header(\"Choose from the options on the sidebar\")\n",
        "  if page == \"Generate Questions\":\n",
        "    # Read in the train set data\n",
        "    train_set  = pd.read_csv('/content/drive/My Drive/KBS/Question_Generator/Train_and_test_data/train.txt',sep=' ',names=['word','Brill','tag']).drop('Brill',1)\n",
        "    train_set = [[tuple(x) for x in train_set.values]]\n",
        "\n",
        "    # Read the Test data from txt\n",
        "    sentence = ' '.join(open('/content/drive/My Drive/KBS/Question_Generator/Text_Passages2.txt','r').readlines()).rstrip(\"\\n\")\n",
        "\n",
        "    def process_text(passage):\n",
        "      exclude_set = set(['“','”',':'])\n",
        "\n",
        "      no_punctuation = [char for char in passage if char not in string.punctuation + \"“”.\"]\n",
        "      no_punctuation = ''.join(no_punctuation)\n",
        "      no_punctuation = [word for word in no_punctuation.split() if word.lower() not in stopwords.words('english')]\n",
        "      \n",
        "      return no_punctuation\n",
        "\n",
        "    nltk.download('stopwords')\n",
        "    #Removing stop words and punctuation\n",
        "    sentence = process_text(sentence)\n",
        "    sentence = ' '.join(sentence)  \n",
        "\n",
        "    nltk.download('punkt')\n",
        "\n",
        "    sentences = sent_tokenize(sentence)\n",
        "    word_list = [[]]\n",
        "    word_list.clear()\n",
        "    wlist = []\n",
        "    i = 0\n",
        "    j = 0\n",
        "\n",
        "    for sent in sentences:\n",
        "        for word in sent.split():\n",
        "            wlist.append(word)\n",
        "            j += 1\n",
        "            \n",
        "        wlist.copy\n",
        "        word_list.append(wlist.copy())\n",
        "        wlist.clear()\n",
        "        i += 1\n",
        "\n",
        "    #Set the model of previously trained data set\n",
        "    ct.set_model_file(model_file='/content/drive/My Drive/KBS/Question_Generator/model.crf.tagger')\n",
        "    word_list = ct.tag_sents(word_list)\n",
        "\n",
        "    test_set = pd.read_csv('/content/drive/My Drive/KBS/Question_Generator/Train_and_test_data/test.txt', sep=' ', names=['words','Brill','tag'], index_col=False).drop(['Brill','tag'],1)\n",
        "    test_set = test_set.values.tolist()\n",
        "    ct.tag_sents(test_set)\n",
        "\n",
        "    test_setEval = pd.read_csv('/content/drive/My Drive/KBS/Question_Generator/Train_and_test_data/test.txt', sep=' ', names=['words','Brill','tag']).drop('Brill',1)\n",
        "    test_setEval = [[tuple(x) for x in test_setEval.values]]\n",
        "    #test the accuracy\n",
        "    ct.evaluate(test_setEval)\n",
        "\n",
        "    #Convert list of tuples to a numpy array\n",
        "    word_list = np.array(word_list)\n",
        "    word_list = np.reshape(word_list, (-1,2))\n",
        "\n",
        "    # Find S -> NP VP NP\n",
        "    # Returns if a subject is found\n",
        "    # pattern variable = [pat0, pat1, pat2]\n",
        "    def findSubject(pattern):\n",
        "        if pattern[0] == 'NP' and pattern[1] == 'VP' and pattern[2] == 'NP':\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "    #Extract the Subject -> NP VP NP to find the subjects in a sentence\n",
        "    #using Markov Chain Model\n",
        "\n",
        "    pattern = []\n",
        "    subjects = []\n",
        "    word_list_length = len(word_list)\n",
        "\n",
        "    for i in range(0, len(word_list)):\n",
        "        #print(word_list[i][1].split('-')[1])\n",
        "        try:\n",
        "            if((word_list_length - i == 2)):\n",
        "                break\n",
        "            else:\n",
        "                pattern = [word_list[i][1].split('-')[1], word_list[i+1][1].split('-')[1], word_list[i+2][1].split('-')[1]]\n",
        "                # Call the function findSubject to identify potential subject elements and save them in an array\n",
        "                if findSubject(pattern):\n",
        "                    # If returns true consider 1st element as a potential Subject\n",
        "                    print('Potential Subject Found at index {}'.format(i))\n",
        "                    print('Subject -> {} => {} {}'.format(word_list[i][0], word_list[i+1][0], word_list[i+2][0]))\n",
        "                    # Put the phrases in to a sentece and append it to an array.\n",
        "                    sub = [word_list[i][0],word_list[i+1][0],word_list[i+2][0]]\n",
        "                    subjects.append(' '.join(sub))\n",
        "                    print('--------------------')\n",
        "        # Exception handler\n",
        "        except IndexError:\n",
        "            print(\"Out of Index\")\n",
        "            break;\n",
        "    # Find PERSONs in the filtered sentence using spaCy NER and build a DataFrame Object based on the data\n",
        "    # DataFrame structure -> Word | POS_TAG | Person\n",
        "    subjects = np.array(subjects)\n",
        "    subjects = np.reshape(subjects, (-1,1))\n",
        "\n",
        "    refferingDataFrame = pd.DataFrame(columns=('Word_POS-TAG_Person','nullColumn'))\n",
        "\n",
        "    for i in range(0,len(subjects)):\n",
        "        for j in range(0,len(subjects[i][0].split(\" \"))):\n",
        "            wordBag = subjects[i][0].split(\" \")\n",
        "            pos = ct.tag([wordBag[j]])\n",
        "            person = [ent.label_ for ent in nlp(pos[0][0]).ents]\n",
        "            person = ' '.join(person)\n",
        "            if not person:\n",
        "                person = \"\"\n",
        "                \n",
        "            #Add a row to the DataFrame with the retrived data\n",
        "            refferingDataFrame.loc[len(refferingDataFrame)] = [(pos[0][0] +\"_\"+ pos[0][1].split('-')[1] +\"_\"+ person).rstrip('_'), \"\"]\n",
        "\n",
        "    # The RNN is created using PyTorch Framework. \n",
        "    #The RNN will clasify the Question according the the category\n",
        "    lstm = nn.LSTM(3, 3)\n",
        "    inputs = [autograd.Variable(torch.randn(1, 3)) for _ in range(5)]\n",
        "    hidden = (autograd.Variable(torch.randn(1, 1, 3)), autograd.Variable(torch.randn(1, 1, 3)))\n",
        "\n",
        "    for i in inputs:\n",
        "        out, hidden = lstm(i.view(1, 1, -1), hidden)\n",
        "        \n",
        "    inputs = torch.cat(inputs).view(len(inputs), 1, -1)\n",
        "    hidden = (autograd.Variable(torch.randn(1, 1, 3)), autograd.Variable(torch.randn(1, 1, 3)))\n",
        "    out, hidden = lstm(inputs, hidden)\n",
        "    #print(out)\n",
        "    #print(hidden)\n",
        "\n",
        "    def prepare_sequence(seq, to_ix):\n",
        "        idxs = [to_ix[w] for w in seq]\n",
        "        tensor = torch.LongTensor(idxs)\n",
        "        return autograd.Variable(tensor)\n",
        "\n",
        "    # Read training data from the csv\n",
        "    # Trained data in the format => [([\"Phrase\"], [\"Question\"])]\n",
        "    unprocessed_data = pd.read_csv('/content/drive/My Drive/KBS/Question_Generator/Train_and_test_data/LSTM_train_set.csv',header=None)\n",
        "\n",
        "    # Transform the data in to a processable format\n",
        "    t_data_for_lstm = []\n",
        "    training_data = []\n",
        "    for phrase in unprocessed_data.itertuples():\n",
        "        t_data_for_lstm.append(list(zip([[phrase[1]]], [[phrase[2]]])))\n",
        "\n",
        "    for i in range(0, len(t_data_for_lstm)):\n",
        "        training_data.append(t_data_for_lstm[i][0])\n",
        "        \n",
        "    word_to_ix = {}\n",
        "    tag_to_ix = {}\n",
        "\n",
        "    for sent, tags in training_data:\n",
        "        for word in sent:\n",
        "            if word not in word_to_ix:\n",
        "                word_to_ix[word] = len(word_to_ix)\n",
        "        for word1 in tags:\n",
        "            if word1 not in tag_to_ix:\n",
        "                tag_to_ix[word1] = len(tag_to_ix)\n",
        "                                \n",
        "    EMBEDDING_DIM = 6\n",
        "    HIDDEN_DIM = 6\n",
        "\n",
        "    # The LSTM model\n",
        "    class LSTMTagger(nn.Module):\n",
        "        def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
        "            super(LSTMTagger, self).__init__()\n",
        "            self.hidden_dim = hidden_dim\n",
        "            \n",
        "            self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "            self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
        "            self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
        "            self.hidden  = self.init_hidden()\n",
        "            \n",
        "        def init_hidden(self):\n",
        "            return (autograd.Variable(torch.zeros(1, 1, self.hidden_dim)), autograd.Variable(torch.zeros(1, 1, self.hidden_dim)))\n",
        "            \n",
        "        def forward(self, sentence):\n",
        "            embeds = self.word_embeddings(sentence)\n",
        "            lstm_out, self.hidden = self.lstm(embeds.view(len(sentence), 1, -1), self.hidden)\n",
        "            tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
        "            tag_scores = F.log_softmax(tag_space)\n",
        "            return tag_scores\n",
        "        \n",
        "    # Setting the variables to Train the model\n",
        "    model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
        "    loss_function = nn.NLLLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "    #Values Before Training\n",
        "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
        "    tag_scores = model(inputs)\n",
        "    t0 = time.time()\n",
        "    iterations = 300\n",
        "    progress_after = iterations/4\n",
        "\n",
        "    # Training the RNN for 300 iterations\n",
        "    for epoch in range(0,iterations):\n",
        "        for sentence, tags in training_data:\n",
        "            model.zero_grad()\n",
        "            model.hidden = model.init_hidden()\n",
        "            sentence_in = prepare_sequence(sentence, word_to_ix)\n",
        "            targets = prepare_sequence(tags, tag_to_ix)\n",
        "            # % of completion\n",
        "            if(progress_after == epoch):\n",
        "                st.write(\"Training {}% Completed\".format((progress_after / iterations) * 100))\n",
        "                progress_after = progress_after + (iterations / 4)\n",
        "                \n",
        "            tag_scores = model(sentence_in)\n",
        "            \n",
        "            loss = loss_function(tag_scores, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "    st.write(\"Training Complete!\\nTotal Training time :\", round(time.time()-t0, 2), \"s\\n\")\n",
        "\n",
        "    #Values after training        \n",
        "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
        "    tag_scores = model(inputs)\n",
        "\n",
        "    #print(\"Values After training\")\n",
        "    #print(tag_scores)\n",
        "\n",
        "    st.markdown('**The questions are;**')\n",
        "    # Formatting the result generated by the LSTM by adding the relevant Noun Phrases\n",
        "    def generate_questions(model, ref_data_frame):\n",
        "        question = \"\"\n",
        "        sentence = \"\"\n",
        "        index = 0\n",
        "        for index, word_phrase in ref_data_frame.iterrows():\n",
        "            # Predict the potentian sentence structure that can be used to generate the question.\n",
        "            try:\n",
        "                inputs = prepare_sequence([word_phrase[0]], word_to_ix)\n",
        "                tag_scores = model(inputs)\n",
        "            except KeyError:\n",
        "                # ignore\n",
        "                pass\n",
        "\n",
        "            # Take the maximum probalilty, and based on the probabilty find index of value of the dictonary\n",
        "            maxVal = max(tag_scores.data.numpy()[0])\n",
        "            index_loc = 0\n",
        "            probability_tag_scores = tag_scores.data.numpy().ravel()\n",
        "\n",
        "            for i in range(0, len(probability_tag_scores)):\n",
        "                if (probability_tag_scores.ravel()[i] == maxVal):\n",
        "                    index_loc = i\n",
        "\n",
        "            # Travers the dictonary and identify the key value based on the probability predicted\n",
        "            for key, value in tag_to_ix.items():\n",
        "                if (value == index_loc):\n",
        "                    # Formatting the Key to generate a meaningful question\n",
        "                    # Extracting subject and object of the tested sentence\n",
        "                    sentence = word_phrase[0].split('_')\n",
        "                    # Checking whether the question needs modification\n",
        "                    if \"NP\" in sentence:\n",
        "                        question = key.replace(\"NP\", sentence[0])\n",
        "                        st.write(question)\n",
        "                    elif \"VP\" in sentence:\n",
        "                        if \"NP\" in key and \"N1\" in key:\n",
        "                            # Finding the complete sentence matching for the verb and index in subjects which matches for verb\n",
        "                            index = [i for i,j in enumerate(subjects.ravel()) if sentence[0] in j]\n",
        "                            ref_subject = subjects.ravel()[index][0].split()\n",
        "                            # Replace for words NP and NP1\n",
        "                            question = key.replace(\"NP\", ref_subject[0]) \n",
        "                            question = question.replace(\"N1\",ref_subject[-1])\n",
        "                            st.write(question)\n",
        "                            \n",
        "                        else:\n",
        "                            # If \"VP\" but only one \"NP\"\n",
        "                            index = [i for i,j in enumerate(subjects.ravel()) if sentence[0] in j]\n",
        "                            ref_subject = subjects.ravel()[index][0].split()\n",
        "                            question = key.replace(\"NP\", ref_subject[0])\n",
        "                            st.write(question)\n",
        "\n",
        "    generate_questions(model, refferingDataFrame)  \n",
        "\n",
        "if __name__ == '__main__':\n",
        "  main()"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting question_gen.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XY-39v_Cze5a"
      },
      "source": [
        "!streamlit run --server.port 80 question_gen.py&>/dev/null&"
      ],
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHgqWj9Tzlq6"
      },
      "source": [
        "address = ngrok.connect(port=8501)"
      ],
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RuwRXJY3zphh",
        "outputId": "3544d95a-7b07-4734-d086-0028b9c656b3"
      },
      "source": [
        "address"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<NgrokTunnel: \"http://9b2d6cdf4c8b.ngrok.io\" -> \"http://localhost:80\">"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shJKW2SczuZC"
      },
      "source": [
        "**Run the code below when more than 2 tunnels are running**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4hOqn_-zqhp"
      },
      "source": [
        "ngrok.kill()"
      ],
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YffeFk151Y3n"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}